# Running Benchmark Tests

This project includes a suite of benchmark tests designed to evaluate the performance and capabilities of the LLM agents using the `deepeval` library. These tests cover various aspects like data transformation, file system organization, time-based queries, code capabilities, and general search/response quality.

## Prerequisites

1.  **Install Dependencies:** Ensure all project dependencies, including those specific to testing and `deepeval`, are installed using `conda`.
    *   **Create/Update Environment:** If you have an `environment.yml` file defining the conda environment, use it to create or update the environment:
        ```bash
        # Create the environment if it doesn't exist
        conda env create -f environment.yml

        # Or update an existing environment
        conda env update --file environment.yml --prune
        ```
    *   **Activate Environment:** Make sure to activate the correct conda environment before running tests:
        ```bash
        conda activate <your-environment-name>
        ```
        *(Replace `<your-environment-name>` with the actual name of your conda environment)*

2.  **Configure Environment:**
    *   Copy the `.env.example` file to `.env` if you haven't already.
    *   Verify the settings in your `.env` file (#.env), especially:
        *   `MODEL_PROVIDER`, `MODEL`, OPENAI_API_KEY (if using OpenAI),
        *   `EMBEDDING_MODEL`, `EMBEDDING_DIM`
        *   `PG_CONNECTION_STRING`, `PSYCOPG2_CONNECTION_STRING`, `DB_NAME`

3. **Generate Embeddings**
    Ensure your PostgreSQL database is running and accessible with the credentials specified in `.env.` Then Initialize vector store database by runing the script:
    ```bash
    ./scripts/run.sh generate
    ```

4.  **Run Backend Server:** The benchmark tests interact with the application's API. Make sure the main FastAPI server is running within your activated conda environment:
    ```bash
    ./scripts/run.sh dev
    ```
    *(Adjust the command based on your setup if needed)*

## Running Tests

The benchmark tests are organized into subdirectories within `tests/benchmark_tests/`. There is a script that will run through generating any data and tests. Each subdirectory also contains a main Python script that runs the tests for that individual category.



**Examples:**

**Run All Tests:**
    ```bash
    ./scripts/run_benchmarks.sh
    ```

**File System Organization Tests:**
    ```bash
    python -m tests.benchmark_tests.file_system_tests.test_organization
    ```

*   **Time-Based Query Tests:**
    ```bash
    python -m tests.benchmark_tests.time_tests.test_time_queries
    ```

*   **Data Transformation Tests:**
    ```bash
    python -m tests.benchmark_tests.data_transformation_tests.test_transformation
    ```

*   **Synthetic Data (Summarization) Tests:**
    ```bash
    python -m tests.benchmark_tests.synthetic_data.test_synthetic_data
    ```

*   **Miscellaneous Tests:**
    ```bash
    python -m tests.benchmark_tests.misc_tests.misc_tests
    ```

*(Refer to the specific scripts within each `tests/benchmark_tests/*_tests/` directory for details on their execution and any specific requirements.)*

## Test Categories

*   **`data_transformation_tests`**: Evaluates the LLM's ability to extract and reformat data based on instructions. 
*   **`file_system_tests`**: Tests the agent's capability to understand and suggest file organization structures. 
*   **`misc_tests`**: Contains various other tests evaluating different agent interactions or capabilities. They were generated by deepeval from the documents, and were made to focus on search capabilities.
*   **`search_tests`**: Focuses on evaluating the quality and relevance of search results retrieved by the agent.
*   **`synthetic_data`**: Uses synthetically generated data (like code summaries) to test specific capabilities. 
*   **`time_tests`**: Evaluates reasoning about time, dates, and file timestamps. 
*   **`test_code_capability.py`**: Tests the generation of code content. To be expanded on to test queries related to code files.

## Output

The tests typically print evaluation results, including scores and reasoning from `deepeval` metrics, directly to the console. Some tests might generate report files or visualizations in their respective directories or a common `results` folder (check individual scripts for details, e.g., #tests/benchmark_tests/imagedataset (1).py defines `RESULTS_DIR`).