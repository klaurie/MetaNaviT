# The provider for the AI models to use.
MODEL_PROVIDER=openai

# The name of LLM model to use.
MODEL=gpt-4o-mini-2024-07-18
OLLAMA_BASE_URL=http://localhost:11434

# Name of the embedding model to use.
EMBEDDING_MODEL=text-embedding-3-large

# Dimension of the embedding model to use.
EMBEDDING_DIM=3068

# The questions to help users get started (multi-line).
# CONVERSATION_STARTERS=

# The OpenAI API key to use.
OPENAI_API_KEY=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
#GOOGLE_API_KEY=AIzaSyADH7LTqQraG0vUTsP4Smq_Tl19xSeg1jw
# Temperature for sampling from the model.
# LLM_TEMPERATURE=

# Maximum number of tokens to generate.
# LLM_MAX_TOKENS=

# The number of similar embeddings to return when retrieving documents.
# TOP_K=

# PostGreSQL database URL.
PG_CONNECTION_STRING="postgresql://postgres:password@localhost:5432/metanavit"
PSYCOPG2_CONNECTION_STRING="dbname=metanavit user=postgres password=password host=localhost port=5432"
DB_NAME="metanavit"

# Storage management
FRONTEND_DIR=".frontend"
STATIC_DIR="static"
DATA_DIR="data/"
STORAGE_DIR="storage"


# The directory to store the local storage cache.
STORAGE_CACHE_DIR=.cache

# FILESERVER_URL_PREFIX is the URL prefix of the server storing the images generated by the interpreter.
FILESERVER_URL_PREFIX=http://localhost:8000/api/files

# The address to start the backend app.
APP_HOST=0.0.0.0

# The port to start the backend app.
APP_PORT=8000

# Customize prompt to generate the next question suggestions based on the conversation history.
# Disable this prompt to disable the next question suggestions feature.
NEXT_QUESTION_PROMPT="You're a helpful assistant! Your task is to suggest the next question that user might ask. 
Here is the conversation history
---------------------
{conversation}
---------------------
Given the conversation history, please give me 3 questions that user might ask next!
Your answer should be wrapped in three sticks which follows the following format:
```
<question 1>
<question 2>
<question 3>
```"

# The system prompt for the AI model.
SYSTEM_PROMPT="You are a helpful AI file system assistant. Your task is to help the user understand and perform tasks related to their files."
